{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fooof import FOOOFGroup\n",
    "from fooof.analysis import get_band_peak_fg\n",
    "import mne"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a13a709172873a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Directory paths\n",
    "csv_load_path = '00_Subjects_Info'\n",
    "epoch_load_path = '02_Epoch_EEG/Concatenated'\n",
    "fooof_load_path = '03_FOOOF_EEG/Concatenated'\n",
    "\n",
    "csv_save_path = '04_Features_EEG/Range_13_10_Sec_Epoch_Fixed.csv'\n",
    "cleaned_csv_path = '04_Features_EEG/Range_13_10_Sec_Epoch_Fixed_Clean.csv'\n",
    "merged_csv_path = '04_Features_EEG/Range_13_10_Sec_Epoch_Fixed_Merged.csv'\n",
    "model_fit_plot_save_folder = '05_Plots/FOOOF_Fit'"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Subjects to include in the analysis\n",
    "EEG_INFO = {\n",
    "    'SUBJ_NUMS': ['10002', '10003', '10004', '10008', '10011', '10012', '10016', '10019', \n",
    "                  '10021', '10022', '10023', '10024', '10028', '10031', '10032', '10034', \n",
    "                  '10036', '10037', '10039', '10040', '10046', '10048', '10056', '10058'],\n",
    "}\n",
    "SUBJ_NUMS = EEG_INFO['SUBJ_NUMS']\n",
    "\n",
    "# Frequency bands of interest (in Hz)\n",
    "BANDS = {\n",
    "    'Delta': (1, 4),\n",
    "    'Theta': (4, 8),\n",
    "    'Alpha': (8, 13)\n",
    "}\n",
    "\n",
    "# EEG channel names\n",
    "CHANNELS = ['F7', 'Fp1', 'Fp2', 'F8', 'F3', 'Fz', 'F4', 'C3', 'Cz', 'P8', 'P7', 'Pz', 'P4', 'T3', 'P3', 'O1', 'O2', 'C4', 'T4']\n",
    "\n",
    "# Channel groupings by anatomical region\n",
    "All_Channel_indices = np.arange(len(CHANNELS))\n",
    "Occipital_Channel_indices = [CHANNELS.index('O1'), CHANNELS.index('O2')]\n",
    "Frontal_Channel_indices = [CHANNELS.index('F7'), CHANNELS.index('F3'), CHANNELS.index('Fz'), CHANNELS.index('F4'), CHANNELS.index('F8')]\n",
    "Central_Channel_indices = [CHANNELS.index('Cz'), CHANNELS.index('C3'), CHANNELS.index('C4')]\n",
    "\n",
    "# Dictionary of channel groups with their corresponding indices\n",
    "Channel_Groups = {\n",
    "    'All': All_Channel_indices,\n",
    "    'Occipital': Occipital_Channel_indices, \n",
    "    'Frontal': Frontal_Channel_indices,\n",
    "    'Central': Central_Channel_indices,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a79bc051f05b8541",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Get FOOOF filenames\n",
    "all_fooof_files = os.listdir(fooof_load_path)\n",
    "\n",
    "# Filter files that end with '_fooof_results_fixed.json'\n",
    "filtered_files = [file for file in all_fooof_files if file.endswith('_fooof_results_fixed.json')]\n",
    "\n",
    "## Load .csv containing session information\n",
    "info_fname = 'Subjects_Info.csv'\n",
    "full_path = os.path.join(csv_load_path, info_fname)\n",
    "session_info_data = pd.read_csv(full_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "107b7de008a8a768",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loop through each subject\n",
    "for subj_idx, subj_num in enumerate(SUBJ_NUMS):\n",
    "    # Filter files that start with the current subject number\n",
    "    subj_files = [file for file in filtered_files if file.startswith(subj_num)]\n",
    "\n",
    "    # Extract session information for each subject\n",
    "    session_info_list = []\n",
    "    for filename in subj_files:\n",
    "        session_info = filename.split(subj_num + '_')[1].split('_fooof_results_fixed.json')[0]\n",
    "        session_info_list.append(session_info)\n",
    "    \n",
    "    # Process each session for the current subject\n",
    "    for sess_idx, sess_num in enumerate(session_info_list):\n",
    "        try:\n",
    "            full_sess_name = str(subj_num) + \"_\" + str(sess_num)\n",
    "            params_grouped = {}  # Dictionary to store computed parameters\n",
    "        \n",
    "            # Load epoch .fif file\n",
    "            epoch = mne.read_epochs(epoch_load_path + '/' + full_sess_name + '_epo.fif', preload=False)\n",
    "\n",
    "            # Calculate duration of session\n",
    "            start_time = epoch.times[0]\n",
    "            end_time = epoch.times[-1]\n",
    "            duration_seconds = end_time - start_time\n",
    "            print(\"Duration of the epoch:\", duration_seconds, \"seconds\")\n",
    "            params_grouped['Duration_Secs'] = duration_seconds\n",
    "        \n",
    "            # Load FOOOF model results\n",
    "            json_filename = os.path.join(fooof_load_path, f'{subj_num}_{sess_num}_fooof_results_fixed.json')\n",
    "            print(json_filename)\n",
    "            fg = FOOOFGroup()\n",
    "            fg.load(json_filename)\n",
    "\n",
    "            # Extract frequency and power spectrum density values\n",
    "            freqs = fg.freqs\n",
    "            psd = fg.power_spectra\n",
    "        \n",
    "            # Extract model fit parameters\n",
    "            offset = fg.get_params('aperiodic_params', 'offset')\n",
    "            exponent = fg.get_params('aperiodic_params', 'exponent')\n",
    "        \n",
    "            # Get R2 and Error values\n",
    "            R2 = fg.get_params('r_squared')\n",
    "            Error = fg.get_params('error')\n",
    "        \n",
    "            # Filter channels based on R2 threshold (>= 0.95)\n",
    "            valid_indices = np.where(R2 >= 0.95)[0]\n",
    "            invalid_indices = np.where(R2 < 0.95)[0]\n",
    "\n",
    "            # Compute and store metrics for all channels\n",
    "            All_R2 = np.nanmean(R2[valid_indices])\n",
    "            All_Error = np.nanmean(Error[valid_indices])\n",
    "            params_grouped['All_R2'] = All_R2\n",
    "            params_grouped['All_Error'] = All_Error\n",
    "            \n",
    "            # Process each channel group\n",
    "            for group_name, channel_indices in Channel_Groups.items():\n",
    "                valid_channel_indices = [idx for idx in channel_indices if idx in valid_indices]\n",
    "                \n",
    "                # Compute group-level aperiodic parameters\n",
    "                group_offset = np.nanmean([offset[idx] for idx in valid_channel_indices])\n",
    "                group_exponent = np.nanmean([exponent[idx] for idx in valid_channel_indices])\n",
    "                params_grouped[f\"{group_name}_Offset\"] = group_offset\n",
    "                params_grouped[f\"{group_name}_Exponent\"] = group_exponent\n",
    "            \n",
    "                # Compute metrics for each frequency band\n",
    "                for band_name, freq_range in BANDS.items():\n",
    "                    # Compute aperiodic-corrected SNR\n",
    "                    SNR = np.zeros(len(valid_channel_indices))\n",
    "                    for i, chan_idx in enumerate(valid_channel_indices):\n",
    "                        ap_fit = fg.get_fooof(chan_idx).get_params('aperiodic_params')\n",
    "                        psd_corr = 10 * psd[chan_idx] - 10 * fg.get_fooof(chan_idx)._ap_fit\n",
    "                        idx_band = np.where((freqs >= freq_range[0]) & (freqs <= freq_range[1]))[0]\n",
    "                        SNR[i] = np.max(psd_corr[idx_band])\n",
    "                    SNR_mean = np.nanmean(SNR)\n",
    "                    params_grouped[f\"{group_name}_{band_name}_SNR\"] = SNR_mean\n",
    "                    \n",
    "                    # Compute power spectrum features\n",
    "                    periodic_feats = get_band_peak_fg(fg, freq_range)\n",
    "                    cf = np.nanmean(periodic_feats[valid_channel_indices, 0])\n",
    "                    pw = np.nanmean(periodic_feats[valid_channel_indices, 1])\n",
    "                    bw = np.nanmean(periodic_feats[valid_channel_indices, 2])\n",
    "                    params_grouped[f\"{group_name}_{band_name}_CF\"] = cf\n",
    "                    params_grouped[f\"{group_name}_{band_name}_PW\"] = pw    \n",
    "                    params_grouped[f\"{group_name}_{band_name}_BW\"] = bw  \n",
    "                    \n",
    "            # Store individual channel exponents\n",
    "            exponent[invalid_indices] = np.nan\n",
    "            for i, channel in enumerate(CHANNELS):\n",
    "                params_grouped[f\"{channel}_Exponent\"] = exponent[i]\n",
    "            \n",
    "            # Convert computed parameters to DataFrame\n",
    "            df_params_grouped = pd.DataFrame([params_grouped])\n",
    "            column_names = list(df_params_grouped.columns.values)\n",
    "            \n",
    "            # Update session info data with computed parameters\n",
    "            subj_num_convert = session_info_data['Record ID'].dtype.type(subj_num)\n",
    "            sess_num_convert = session_info_data['Event Name'].dtype.type(sess_num)\n",
    "            index = session_info_data[(session_info_data['Record ID'] == subj_num_convert) &\n",
    "                                      (session_info_data['Event Name'] == sess_num_convert)].index\n",
    "            session_info_data.loc[session_info_data.index[index], column_names] = df_params_grouped.values\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {subj_num}, session {sess_num}: {e}\")\n",
    "            continue\n",
    "              \n",
    "# Save the updated DataFrame back to the CSV file\n",
    "session_info_data.to_csv(csv_save_path, index=False, mode='w+', header=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95273d134e96c9c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Clean dataframe to include only the sessions with highest occipital alpha SNR when there are retries due to child not cooperating\n",
    "\n",
    "# Create a copy of the original DataFrame to preserve the original data\n",
    "df_copy = session_info_data.copy()\n",
    "\n",
    "# Remove sessions where no EEG was recorded\n",
    "# This is done by dropping rows where 'Occipital_Alpha_SNR' is NaN\n",
    "df = df_copy.dropna(subset=['Occipital_Alpha_SNR'])\n",
    "\n",
    "# Sort the DataFrame by 'Record ID', 'Timepoint', and 'Occipital_Alpha_SNR'\n",
    "# - 'Record ID' and 'Timepoint' are sorted in ascending order (True)\n",
    "# - 'Occipital_Alpha_SNR' is sorted in descending order (False) to have highest SNR first\n",
    "df_sorted = df.sort_values(by=['Record ID', 'Timepoint', 'Occipital_Alpha_SNR'], \n",
    "                           ascending=[True, True, False])\n",
    "\n",
    "# Drop duplicates, keeping only the first entry (highest 'Occipital_Alpha_SNR') \n",
    "# within each 'Record ID' + 'Timepoint' group\n",
    "df_highest_snr_kept = df_sorted.drop_duplicates(subset=['Record ID', 'Timepoint'], keep='first')\n",
    "\n",
    "# Perform a final check for any remaining duplicates\n",
    "# This creates a boolean Series where True indicates a duplicate\n",
    "remaining_duplicates_check = df_highest_snr_kept.duplicated(subset=['Record ID', 'Timepoint'], keep=False)\n",
    "\n",
    "# Save the final cleaned DataFrame to a new CSV file\n",
    "# 'index=False' prevents writing row indices to the CSV\n",
    "# 'mode='w+'' opens the file for reading and writing, creating it if it doesn't exist\n",
    "df_highest_snr_kept.to_csv(cleaned_csv_path, index=False, mode='w+')\n",
    "\n",
    "# Create a dictionary with the cleaning results\n",
    "# This includes the path to the cleaned CSV and whether any duplicates remain\n",
    "result = {\n",
    "    \"cleaned_csv_path\": cleaned_csv_path,\n",
    "    \"duplicates_remaining\": remaining_duplicates_check.any()  # True if any duplicates remain, False otherwise\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cab9631b9ce5d328",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge EEG data with Behavior and CBD level information\n",
    "\n",
    "# Load the CSV file containing behavior and CBD level data\n",
    "cbd_levels_df = pd.read_csv('00_Subjects_Info/Subjects_Info_Behavior_CBD_Levels.csv')\n",
    "\n",
    "# Load the cleaned EEG session data (that has no duplicates)\n",
    "session_info_df = pd.read_csv(cleaned_csv_path)\n",
    "\n",
    "# Define the columns to be added from the CBD and behavior data\n",
    "columns_to_concat = [\n",
    "    'Time Difference (eegassessment-cbdlevel)', 'CBD', 'OHCBD', 'COOHCBD', 'AEA',\n",
    "    'rbs_stereotyped_behavior_subscale', 'rbs_self_injurious_behavior_subscale',\n",
    "    'rbs_complusive_behavior_subscale', 'rbs_ritualistic_behavior_subscale',\n",
    "    'rbs_sameness_behavior_subscale', 'rbs_restricted_behavior_subscale',\n",
    "    'rbs_total_score', 'ppvt_raw_score', 'ppvt_standard_score',\n",
    "    'toni4_raw_score', 'toni4_index_score', 'eowpvt4_raw_score',\n",
    "    'eowpvt4_standard_score', 'beery_vmi_raw_score', 'beery_vmi_standard_score',\n",
    "    'beery_vp_raw_score', 'beery_vp_standard_score', 'beery_mc_raw_score',\n",
    "    'beery_mc_standard_score'\n",
    "]\n",
    "\n",
    "# Perform an inner join to merge the datasets\n",
    "# This keeps only the rows that have matching 'Record ID' and 'Timepoint' in both datasets\n",
    "merged_df = session_info_df.merge(\n",
    "    cbd_levels_df[['Record ID', 'Timepoint'] + columns_to_concat],\n",
    "    on=['Record ID', 'Timepoint'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Remove any duplicate rows based on 'Record ID', 'Event Name', 'Randomization', and 'Timepoint'\n",
    "# If duplicates exist, keep the last occurrence\n",
    "merged_df.drop_duplicates(['Record ID', 'Event Name', 'Randomization', 'Timepoint'], \n",
    "                          keep='last', inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(merged_csv_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3ce1771c310d3e7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
